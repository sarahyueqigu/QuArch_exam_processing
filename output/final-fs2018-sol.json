[
    {
        "question_id": "final-fs2018-sol/Problem_1/a",
        "context": "Researchers at Lindtel developed a new memory technology, L-RAM, which is non-volatile. The access latency of L-RAM is close to that of DRAM while it provides higher density compared to the latest DRAM technologies. L-RAM has one shortcoming, however: it has limited endurance, i.e., a memory cell stops functioning after 10^6 writes are performed to the cell (known as cell wear-out).\nLindtel markets a new computer system with L-RAM to have a lifetime of 2 years and the following specifications:\n\n\u2022 4GBs of L-RAM as main memory with a perfect wear-leveling mechanism, i.e., writes are equally distributed over all the cells of L-RAM.\n\n\u2022 The processor is in-order and there is no memory-level parallelism.\n\n\u2022 It takes 4 ns to send a memory request from the processor to the memory controller and it takes 20 ns to send the request from the memory controller to L-RAM. The write latency of L-RAM is 40 ns.\n\n\u2022 L-RAM is word-addressable. Thus, each write request writes 8 bytes to memory.",
        "context_figures": [],
        "question": "A student at ETH tests the lifetime of the system and finds that this new computer system cannot guarantee a lifetime of 2 years. She writes a program to wear out the entire L-RAM device as quickly as possible. How fast is she able to wear out the device? Show all work.",
        "solution": "twear_out = (2^32)/(2^3 \u00d7 10^6 \u00d7 (40 + 4 + 20))\ntwear_out = 2^35 \u00d7 10^6 ns\ntwear_out \u2248 397.68 days\n\nExplanation:\n\u2022 Each memory cell should receive 10^6 writes.\n\u2022 Since ETH-RAM is word addressable, the required number of writes is equal to 2^32/2^3 \u00d7 10^6.\n\u2022 The processor is in-order and there is no memory-level parallelism, so the total latency of each memory access is equal to 40 + 4 + 20.",
        "solution_figures": [],
        "correctly_parsed": null
    },
    {
        "question_id": "final-fs2018-sol/Problem_1/b",
        "context": "Researchers at Lindtel developed a new memory technology, L-RAM, which is non-volatile. The access latency of L-RAM is close to that of DRAM while it provides higher density compared to the latest DRAM technologies. L-RAM has one shortcoming, however: it has limited endurance, i.e., a memory cell stops functioning after 10^6 writes are performed to the cell (known as cell wear-out).\nL-RAM works in the multi-level cell (MLC) mode in which each memory cell stores 2 bits. The student decides to improve the lifetime of L-RAM cells by using the single-level cell (SLC) mode. When L-RAM is used in SLC mode, the lifetime of each cell improves by a factor of 10 and the write latency decreases by 75%.",
        "context_figures": [],
        "question": "What is the lifetime of the system using the SLC mode, if we repeat the experiment in part (a), with all else remaining the same in the system? Show your work.",
        "solution": "twear_out = (2^31)/(2^3 \u00d7 10^7 \u00d7 (10 + 4 + 20)\u00d7 10^-9)\ntwear_out = 91268055.04s \u2248 1056.34 days\n\nExplanation:\n\u2022 Each memory cell should receive 10\u00d7 10^6 = 10^7 writes.\n\u2022 The memory capacity is reduced by 50% since we are using SLC: Capacity = 2^32/2 = 2^31\n\u2022 The required number of writes is equal to 2^31/2^3 \u00d7 10^7.\n\u2022 The SLC write latency is 0.25\u00d7 twrite_MLC : twrite_SLC = 0.25\u00d7 40 = 10 ns",
        "solution_figures": [],
        "correctly_parsed": null
    },
    {
        "question_id": "final-fs2018-sol/Problem_1/c",
        "context": "Researchers at Lindtel developed a new memory technology, L-RAM, which is non-volatile. The access latency of L-RAM is close to that of DRAM while it provides higher density compared to the latest DRAM technologies. L-RAM has one shortcoming, however: it has limited endurance, i.e., a memory cell stops functioning after 10^6 writes are performed to the cell (known as cell wear-out).\n",
        "context_figures": [],
        "question": "Provide a mechanism that would increase the guaranteed lifetime of the computer system without changing the physical circuitry of L-RAM. From the baseline computer system in part (a), describe the changes required to guarantee a computer system lifetime of 2 years, with your mechanism. Be concrete and precise.",
        "solution": "Artificially increase the time to either (1) send a memory request from the memory controller to L-RAM or (2) send a request from the processor to the memory controller by 54 ns. 730 \u2217 3600 \u2217 24 < 2^32/2^3 \u00d7 10^6 \u00d7 (40 + 4 + 20 + x)\nx > 53.4ns",
        "solution_figures": [],
        "correctly_parsed": null
    },
    {
        "question_id": "final-fs2018-sol/Problem_2/a",
        "context": "In lectures, we introduced a variety of ways to tackle memory interference. In this problem, we will look at the Blacklisting Memory Scheduler (BLISS) to reduce unfairness. There are two key aspects of BLISS that you need to know.\n\n\u2022 When the memory controller services \u03b7 consecutive requests from a particular application, this application is blacklisted. We name this non-negative integer \u03b7 the Blacklisting Threshold.\n\n\u2022 The blacklist is cleared periodically every 10000 cycles starting at t = 0.\n\nTo reduce unfairness, memory requests in BLISS are prioritized in the following order:\n\n\u2022 Non-blacklisted applications' requests\n\n\u2022 Row buffer hit requests\n\n\u2022 Older requests\n\nThe memory system for this problem consists of 2 channels with 2 banks each. Tables 1 and 2 show the memory request stream in the same bank for both applications at different times (t = 0 and t = 10). For both tables, a request on the left-hand side is older than a request on the right-hand side in the same table. The applications do not generate more requests than those shown in Tables 1 and 2. The memory requests are labeled with numbers that represent the row position of the data within the accessed bank. Assume the following for all questions:\n\n\u2022 A row buffer hit takes 100 cycles.\n\n\u2022 A row buffer miss (i.e., opening a row in a bank with a closed row buffer) takes 200 cycles.\n\n\u2022 A row buffer conflict (i.e., closing the currently open row and opening another one) takes 250 cycles.\n\n\u2022 All row buffers are closed at time t = 0\n\nApplication A (Channel 0, Bank 0)\nApplication B (Channel 0, Bank 0) Row 2 Row 2 Row 2 Row 2 Row 2 Row 3 Row 3 Row 4\n\nTable 1: Memory requests of the two applications at t = 0\n\nApplication A (Channel 0, Bank 0) Row 3 Row 7 Row 2 Row 0 Row 5 Row 3 Row 8 Row 9\nApplication B (Channel 0, Bank 0) Row 2 Row 2 Row 2 Row 2 Row 2 Row 3 Row 3 Row 4\n\nTable 2: Memory requests of the two applications at t = 10. Note that none of the Application B's existing requests are serviced yet.\n",
        "context_figures": [
            "images\\final-fs2018-sol\\chart_p5_0.png",
            "images\\final-fs2018-sol\\chart_p5_1.png"
        ],
        "question": "Compute the slowdown of each application using the FR-FCFS scheduling policy after both threads ran to completion. We define:\n\nslowdown = memory latency of the application when run together with other applications / memory latency of the application when run alone\n\nShow your work.",
        "solution": "slowdownA =\u223c 1.53\nslowdownB = 1.25\n\nExplanation:\nFor both applications, the first request will incur row buffer miss penalty, and the rest of the requests will either be hits or conflicts.\nApplication A (alone) = 200 + 100 + 250 \u2217 6 = 1800 cycles\nApplication B (alone) = 200 + 100 \u2217 4 + 250 + 100 + 250 = 1200 cycles\n\nApplications A (with B, FR-FCFS) = 200+100\u22174+100+250+100+100\u22172+250+250\u22175 = 2750 cycles\nApplications B (with A, FR-FCFS) = 200 + 100 \u2217 4 + 100 + 250 + 100 + 100 \u2217 2 + 250 = 1500 cycles\n\nFrom the two tables above we know that all requests of application B were issued before any of the application A's requests were issued. Thus, all requests of B are prioritized unless there is a row hit for A's requests.\nslowdownA = 2750/1800 =\u223c 1.53\n\nslowdownB = 1500/1200 = 1.25",
        "solution_figures": [],
        "correctly_parsed": null
    },
    {
        "question_id": "final-fs2018-sol/Problem_2/b",
        "context": "In lectures, we introduced a variety of ways to tackle memory interference. In this problem, we will look at the Blacklisting Memory Scheduler (BLISS) to reduce unfairness. There are two key aspects of BLISS that you need to know.\n\n\u2022 When the memory controller services \u03b7 consecutive requests from a particular application, this application is blacklisted. We name this non-negative integer \u03b7 the Blacklisting Threshold.\n\n\u2022 The blacklist is cleared periodically every 10000 cycles starting at t = 0.\n\nTo reduce unfairness, memory requests in BLISS are prioritized in the following order:\n\n\u2022 Non-blacklisted applications' requests\n\n\u2022 Row buffer hit requests\n\n\u2022 Older requests\n\nThe memory system for this problem consists of 2 channels with 2 banks each. Tables 1 and 2 show the memory request stream in the same bank for both applications at different times (t = 0 and t = 10). For both tables, a request on the left-hand side is older than a request on the right-hand side in the same table. The applications do not generate more requests than those shown in Tables 1 and 2. The memory requests are labeled with numbers that represent the row position of the data within the accessed bank. Assume the following for all questions:\n\n\u2022 A row buffer hit takes 100 cycles.\n\n\u2022 A row buffer miss (i.e., opening a row in a bank with a closed row buffer) takes 200 cycles.\n\n\u2022 A row buffer conflict (i.e., closing the currently open row and opening another one) takes 250 cycles.\n\n\u2022 All row buffers are closed at time t = 0\n\nApplication A (Channel 0, Bank 0)\nApplication B (Channel 0, Bank 0) Row 2 Row 2 Row 2 Row 2 Row 2 Row 3 Row 3 Row 4\n\nTable 1: Memory requests of the two applications at t = 0\n\nApplication A (Channel 0, Bank 0) Row 3 Row 7 Row 2 Row 0 Row 5 Row 3 Row 8 Row 9\nApplication B (Channel 0, Bank 0) Row 2 Row 2 Row 2 Row 2 Row 2 Row 3 Row 3 Row 4\n\nTable 2: Memory requests of the two applications at t = 10. Note that none of the Application B's existing requests are serviced yet.\n",
        "context_figures": [
            "images\\final-fs2018-sol\\chart_p5_0.png",
            "images\\final-fs2018-sol\\chart_p5_1.png"
        ],
        "question": "If we use the BLISS scheduler, for what value(s) of \u03b7 (the Blacklisting Threshold) will the slowdowns of both applications be equal to those obtained with FR-FCFS?",
        "solution": "For \u03b7 \u2265 6 or \u03b7 = 0.\n\nExplanation:\nWe want both A and B to complete without blacklisting or to complete both blacklisted, thus \u03b7 \u2265 6 and \u03b7 = 0, respectively.",
        "solution_figures": [],
        "correctly_parsed": null
    },
    {
        "question_id": "final-fs2018-sol/Problem_2/c",
        "context": "In lectures, we introduced a variety of ways to tackle memory interference. In this problem, we will look at the Blacklisting Memory Scheduler (BLISS) to reduce unfairness. There are two key aspects of BLISS that you need to know.\n\n\u2022 When the memory controller services \u03b7 consecutive requests from a particular application, this application is blacklisted. We name this non-negative integer \u03b7 the Blacklisting Threshold.\n\n\u2022 The blacklist is cleared periodically every 10000 cycles starting at t = 0.\n\nTo reduce unfairness, memory requests in BLISS are prioritized in the following order:\n\n\u2022 Non-blacklisted applications' requests\n\n\u2022 Row buffer hit requests\n\n\u2022 Older requests\n\nThe memory system for this problem consists of 2 channels with 2 banks each. Tables 1 and 2 show the memory request stream in the same bank for both applications at different times (t = 0 and t = 10). For both tables, a request on the left-hand side is older than a request on the right-hand side in the same table. The applications do not generate more requests than those shown in Tables 1 and 2. The memory requests are labeled with numbers that represent the row position of the data within the accessed bank. Assume the following for all questions:\n\n\u2022 A row buffer hit takes 100 cycles.\n\n\u2022 A row buffer miss (i.e., opening a row in a bank with a closed row buffer) takes 200 cycles.\n\n\u2022 A row buffer conflict (i.e., closing the currently open row and opening another one) takes 250 cycles.\n\n\u2022 All row buffers are closed at time t = 0\n\nApplication A (Channel 0, Bank 0)\nApplication B (Channel 0, Bank 0) Row 2 Row 2 Row 2 Row 2 Row 2 Row 3 Row 3 Row 4\n\nTable 1: Memory requests of the two applications at t = 0\n\nApplication A (Channel 0, Bank 0) Row 3 Row 7 Row 2 Row 0 Row 5 Row 3 Row 8 Row 9\nApplication B (Channel 0, Bank 0) Row 2 Row 2 Row 2 Row 2 Row 2 Row 3 Row 3 Row 4\n\nTable 2: Memory requests of the two applications at t = 10. Note that none of the Application B's existing requests are serviced yet.\n",
        "context_figures": [
            "images\\final-fs2018-sol\\chart_p5_0.png",
            "images\\final-fs2018-sol\\chart_p5_1.png"
        ],
        "question": "For what value(s) of \u03b7 (the Blacklisting Threshold) will the slowdown of A be < 1.5?",
        "solution": "Impossible. Slowdown for A will always be \u2265 1.5\n\nExplanation: For the give memory requests, it is not possible to find \u03b7 that blacklists B but not A. Thus, the smallest slowdown for A is the case explained in the solution of part (b).",
        "solution_figures": [],
        "correctly_parsed": null
    },
    {
        "question_id": "final-fs2018-sol/Problem_2/d",
        "context": "In lectures, we introduced a variety of ways to tackle memory interference. In this problem, we will look at the Blacklisting Memory Scheduler (BLISS) to reduce unfairness. There are two key aspects of BLISS that you need to know.\n\n\u2022 When the memory controller services \u03b7 consecutive requests from a particular application, this application is blacklisted. We name this non-negative integer \u03b7 the Blacklisting Threshold.\n\n\u2022 The blacklist is cleared periodically every 10000 cycles starting at t = 0.\n\nTo reduce unfairness, memory requests in BLISS are prioritized in the following order:\n\n\u2022 Non-blacklisted applications' requests\n\n\u2022 Row buffer hit requests\n\n\u2022 Older requests\n\nThe memory system for this problem consists of 2 channels with 2 banks each. Tables 1 and 2 show the memory request stream in the same bank for both applications at different times (t = 0 and t = 10). For both tables, a request on the left-hand side is older than a request on the right-hand side in the same table. The applications do not generate more requests than those shown in Tables 1 and 2. The memory requests are labeled with numbers that represent the row position of the data within the accessed bank. Assume the following for all questions:\n\n\u2022 A row buffer hit takes 100 cycles.\n\n\u2022 A row buffer miss (i.e., opening a row in a bank with a closed row buffer) takes 200 cycles.\n\n\u2022 A row buffer conflict (i.e., closing the currently open row and opening another one) takes 250 cycles.\n\n\u2022 All row buffers are closed at time t = 0\n\nApplication A (Channel 0, Bank 0)\nApplication B (Channel 0, Bank 0) Row 2 Row 2 Row 2 Row 2 Row 2 Row 3 Row 3 Row 4\n\nTable 1: Memory requests of the two applications at t = 0\n\nApplication A (Channel 0, Bank 0) Row 3 Row 7 Row 2 Row 0 Row 5 Row 3 Row 8 Row 9\nApplication B (Channel 0, Bank 0) Row 2 Row 2 Row 2 Row 2 Row 2 Row 3 Row 3 Row 4\n\nTable 2: Memory requests of the two applications at t = 10. Note that none of the Application B's existing requests are serviced yet.\n",
        "context_figures": [
            "images\\final-fs2018-sol\\chart_p5_0.png",
            "images\\final-fs2018-sol\\chart_p5_1.png"
        ],
        "question": "For what value(s) of \u03b7 (the Blacklisting Threshold) will B experience the maximum slowdown it can possibly experience with the Blacklisting Scheduler?",
        "solution": "For \u03b7 = 5.\n\nExplanation: We already know that the slowdowns will be equal to the slowdown with FR-FCFS when \u03b7 \u2265 6 or \u03b7 = 0. If we execute the memory requests for the rest of possible \u03b7 values, we find that \u03b7 = 5 causes application B to complete after 2150 cycles, which is the largest.",
        "solution_figures": [],
        "correctly_parsed": null
    },
    {
        "question_id": "final-fs2018-sol/Problem_2/e",
        "context": "In lectures, we introduced a variety of ways to tackle memory interference. In this problem, we will look at the Blacklisting Memory Scheduler (BLISS) to reduce unfairness. There are two key aspects of BLISS that you need to know.\n\n\u2022 When the memory controller services \u03b7 consecutive requests from a particular application, this application is blacklisted. We name this non-negative integer \u03b7 the Blacklisting Threshold.\n\n\u2022 The blacklist is cleared periodically every 10000 cycles starting at t = 0.\n\nTo reduce unfairness, memory requests in BLISS are prioritized in the following order:\n\n\u2022 Non-blacklisted applications' requests\n\n\u2022 Row buffer hit requests\n\n\u2022 Older requests\n\nThe memory system for this problem consists of 2 channels with 2 banks each. Tables 1 and 2 show the memory request stream in the same bank for both applications at different times (t = 0 and t = 10). For both tables, a request on the left-hand side is older than a request on the right-hand side in the same table. The applications do not generate more requests than those shown in Tables 1 and 2. The memory requests are labeled with numbers that represent the row position of the data within the accessed bank. Assume the following for all questions:\n\n\u2022 A row buffer hit takes 100 cycles.\n\n\u2022 A row buffer miss (i.e., opening a row in a bank with a closed row buffer) takes 200 cycles.\n\n\u2022 A row buffer conflict (i.e., closing the currently open row and opening another one) takes 250 cycles.\n\n\u2022 All row buffers are closed at time t = 0\n\nApplication A (Channel 0, Bank 0)\nApplication B (Channel 0, Bank 0) Row 2 Row 2 Row 2 Row 2 Row 2 Row 3 Row 3 Row 4\n\nTable 1: Memory requests of the two applications at t = 0\n\nApplication A (Channel 0, Bank 0) Row 3 Row 7 Row 2 Row 0 Row 5 Row 3 Row 8 Row 9\nApplication B (Channel 0, Bank 0) Row 2 Row 2 Row 2 Row 2 Row 2 Row 3 Row 3 Row 4\n\nTable 2: Memory requests of the two applications at t = 10. Note that none of the Application B's existing requests are serviced yet.\n",
        "context_figures": [
            "images\\final-fs2018-sol\\chart_p5_0.png",
            "images\\final-fs2018-sol\\chart_p5_1.png"
        ],
        "question": "What is a simple mechanism (that we discussed in lectures) that we can use instead of BLISS to make the slowdowns of both A and B equal to 1.00?",
        "solution": "Memory Channel Partitioning (MCP)\n\nExplanation: With MCP, each application will operate on an independent channel, without any interference with the other application.",
        "solution_figures": [],
        "correctly_parsed": null
    },
    {
        "question_id": "final-fs2018-sol/Problem_3/a",
        "context": "A microprocessor manufacturer asks you to design an asymmetric multicore processor for modern workloads. You should optimize it assuming a workload with 80% of its work in the parallel portion. Your design contains one large core and several small cores, which share the same die. Assume the total die area is 32 units.\n\n\u2022 Large core: For a large core that is n times faster than a single small core, you will need n3 units of die area (n is a positive integer). The dynamic power of this core is 6\u00d7 n Watts and the static power is n Watts.\n\n\u2022 Small cores: You will fit as many small cores as possible, after placing the large core. A small core occupies 1 unit of die area. Its dynamic power is 1 Watt and its static power is 0.5 Watts.\n\nThe parallel portion executes only on the small cores, while the serial portion executes only on the large core.\n\nPlease answer the following questions. Show your work. Express your equations and solve them. You can approximate some computations, and get partial or full credit.\n",
        "context_figures": [],
        "question": "What configuration (i.e., number of small cores and size of the large core) results in the best performance?",
        "solution": "One large core and 24 small cores. The large core will occupy 8 units of die area.\n\nExplanation:\nGiven that the large core occupies n3 units, the number of small cores will be 32 \u2212 n3.\nThus, the speedup can be calculated as:\nSpeedup = 1\n0.2\nn + 0.8\n32\u2212n3\n.\n\nWithout loss of generality, we assume that the total execution time is:\nttotal = tserial + tparallel =\n0.2\nn + 0.8\n32\u2212n3 seconds.\n\nn #small tserial tparallel ttotal\n1 31 0.20 0.03 0.23\n2 24 0.10 0.03 0.13\n3 5 0.07 0.16 0.23\n\nThese calculations can be approximated without a calculator:\nn #small tserial tparallel ttotal\n1 31 0.20 / 1 = 0.20 0.02 < 0.80 / 31 < 0.03 > 0.22\n2 24 0.20 / 2 = 0.10 0.03 < 0.80 / 24 < 0.04 < 0.14\n3 5 0.20 / 3 = 0.07 0.80 / 5 = 0.16 > 0.22",
        "solution_figures": [],
        "correctly_parsed": null
    },
    {
        "question_id": "final-fs2018-sol/Problem_3/b",
        "context": "A microprocessor manufacturer asks you to design an asymmetric multicore processor for modern workloads. You should optimize it assuming a workload with 80% of its work in the parallel portion. Your design contains one large core and several small cores, which share the same die. Assume the total die area is 32 units.\n\n\u2022 Large core: For a large core that is n times faster than a single small core, you will need n3 units of die area (n is a positive integer). The dynamic power of this core is 6\u00d7 n Watts and the static power is n Watts.\n\n\u2022 Small cores: You will fit as many small cores as possible, after placing the large core. A small core occupies 1 unit of die area. Its dynamic power is 1 Watt and its static power is 0.5 Watts.\n\nThe parallel portion executes only on the small cores, while the serial portion executes only on the large core.\n\nPlease answer the following questions. Show your work. Express your equations and solve them. You can approximate some computations, and get partial or full credit.\n",
        "context_figures": [],
        "question": "The energy consumption should also be a metric of reference in your design. Compute the energy consumption for the best configuration in part (a).",
        "solution": "Etotal = 26\u00d7 tserial + 38\u00d7 tparallel = 3.74 Joules.\n\nExplanation:\nWe can calculate the energy consumption as:\nEtotal = Elarge + Esmall =\n(Plarge_dynamic + Plarge_static)\u00d7 tserial + Plarge_static \u00d7 tparallel\n+ (Psmall_static \u00d7 tserial + (Psmall_dynamic + Psmall_static)\u00d7 tparallel)\u00d7 (32\u2212 n3) =\n7\u00d7 n\u00d7 tserial + n\u00d7 tparallel + (0.5\u00d7 tserial + 1.5\u00d7 tparallel)\u00d7 (32\u2212 n3) =\n14\u00d7 tserial + 2\u00d7 tparallel + 12\u00d7 tserial + 36\u00d7 tparallel =\n26\u00d7 tserial + 38\u00d7 tparallel = 3.74 Joules.\n\nThis result can be approximated without a calculator:\nEtotal < 26\u00d7 0.10 + 38\u00d7 0.04 = 2.6 + 1.52 = 4.12 Joules.",
        "solution_figures": [],
        "correctly_parsed": null
    },
    {
        "question_id": "final-fs2018-sol/Problem_3/c",
        "context": "A microprocessor manufacturer asks you to design an asymmetric multicore processor for modern workloads. You should optimize it assuming a workload with 80% of its work in the parallel portion. Your design contains one large core and several small cores, which share the same die. Assume the total die area is 32 units.\n\n\u2022 Large core: For a large core that is n times faster than a single small core, you will need n3 units of die area (n is a positive integer). The dynamic power of this core is 6\u00d7 n Watts and the static power is n Watts.\n\n\u2022 Small cores: You will fit as many small cores as possible, after placing the large core. A small core occupies 1 unit of die area. Its dynamic power is 1 Watt and its static power is 0.5 Watts.\n\nThe parallel portion executes only on the small cores, while the serial portion executes only on the large core.\n\nPlease answer the following questions. Show your work. Express your equations and solve them. You can approximate some computations, and get partial or full credit.\nFor the best configuration obtained in part (a), you are considering to use the large core to collaborate with the small cores on the execution of the parallel portion.",
        "context_figures": [],
        "question": "",
        "solution": "",
        "solution_figures": [],
        "correctly_parsed": null
    },
    {
        "question_id": "final-fs2018-sol/Problem_3/d",
        "context": "A microprocessor manufacturer asks you to design an asymmetric multicore processor for modern workloads. You should optimize it assuming a workload with 80% of its work in the parallel portion. Your design contains one large core and several small cores, which share the same die. Assume the total die area is 32 units.\n\n\u2022 Large core: For a large core that is n times faster than a single small core, you will need n3 units of die area (n is a positive integer). The dynamic power of this core is 6\u00d7 n Watts and the static power is n Watts.\n\n\u2022 Small cores: You will fit as many small cores as possible, after placing the large core. A small core occupies 1 unit of die area. Its dynamic power is 1 Watt and its static power is 0.5 Watts.\n\nThe parallel portion executes only on the small cores, while the serial portion executes only on the large core.\n\nPlease answer the following questions. Show your work. Express your equations and solve them. You can approximate some computations, and get partial or full credit.\n",
        "context_figures": [],
        "question": "Now assume that the serial portion can be optimized, i.e., the serial portion becomes smaller. This gives you the possibility of reducing the size of the large core, and still improving performance. For a large core with an area of (n \u2212 1)3, where n is the value obtained in part (a), what should be the fraction of serial portion that would lead to better performance than in part (a)?",
        "solution": "10%.\n\nExplanation:\nWe call ttotal the total execution time with a large core with n = 2, as obtained in part (a), and t\u2032total for a smaller core with n = 1. We can obtain the new parallel fraction p from the following equation:\n\nttotal > t\u2032total;\n\n0.13 > 1\u2212p\nn\u22121 + p\n32\u2212(n\u22121)3 ;\n\n0.13 > 1\u2212p\n1 + p\n31 ;\n\np > 0.90.\n\nThe serial portion should be at most 10%.",
        "solution_figures": [],
        "correctly_parsed": null
    },
    {
        "question_id": "final-fs2018-sol/Problem_3/e",
        "context": "A microprocessor manufacturer asks you to design an asymmetric multicore processor for modern workloads. You should optimize it assuming a workload with 80% of its work in the parallel portion. Your design contains one large core and several small cores, which share the same die. Assume the total die area is 32 units.\n\n\u2022 Large core: For a large core that is n times faster than a single small core, you will need n3 units of die area (n is a positive integer). The dynamic power of this core is 6\u00d7 n Watts and the static power is n Watts.\n\n\u2022 Small cores: You will fit as many small cores as possible, after placing the large core. A small core occupies 1 unit of die area. Its dynamic power is 1 Watt and its static power is 0.5 Watts.\n\nThe parallel portion executes only on the small cores, while the serial portion executes only on the large core.\n\nPlease answer the following questions. Show your work. Express your equations and solve them. You can approximate some computations, and get partial or full credit.\n",
        "context_figures": [],
        "question": "Your design is so successful for desktop processors that the company wants to produce a similar design for mobile devices. The power budget becomes a constraint. For a maximum of total power of 20W, how much would you need to reduce the dynamic power consumption of the large core, if at all, for the best configuration obtained in part (a)? Assume again that the parallel fraction is 80% of the workload. (Hint: Express the dynamic power of the large core as D \u00d7 n Watts, where D is a constant).",
        "solution": "We have to reduce the dynamic power consumption of the large core by at least 20\u00d7.\n\nExplanation:\nWe calculate the total power as the total energy divided by the total execution time:\nPtotal =\nEtotal\nttotal\nWatts;\n\nPtotal =\nElarge+Esmall\nttotal\n\u2264 20 Watts;\n\nWe express the dynamic power of the large core as D \u00d7 n. From part (a) we know n, tserial, tparallel and ttotal, from part (b) we know Esmall:\n\n(D+1)\u00d7n\u00d7tserial+n\u00d7tparallel+Esmall\nttotal\n= (D+1)\u00d72\u00d70.10+n\u00d70.03+2.00\n0.13 \u2264 20 Watts;\n\nD \u2264 0.3.\n\nIn mobile devices, the dynamic power of the large core has to be \u2264 0.3 \u00d7 n Watts (given the assumptions in the question). Since the dynamic power of the large core is 6\u00d7nWatts in the desktop processor, we have to reduce the dynamic power consumption of the large core by at least 20\u00d7 for mobile devices.",
        "solution_figures": [],
        "correctly_parsed": null
    },
    {
        "question_id": "final-fs2018-sol/Problem_4/a",
        "context": "Multicore Cache Partitioning [55 points]\n\nSuppose we have a system with 32 cores that share a physical second-level cache. Assume each core is running a single single-threaded application, and all 32 cores are concurrently running applications. Assume that the page size of the architecture is 8KB, the block size of the cache is 128 bytes, and the cache uses LRU replacement. We would like to ensure each application gets a dedicated space in this shared cache without any interference from other cores. We would like to enforce this using the OS-based page coloring mechanism to partition the cache, as we discussed in lecture. Recall that with page coloring, the operating system ensures, using virtual memory mechanisms, that the applications do not contend for the same space in the cache.\n",
        "context_figures": [],
        "question": "What is the minimum size the L2 cache needs to be such that each application is allocated its dedicated space in the cache via page coloring? Show your work.",
        "solution": "256KB.\n\nExplanation:\nFor OS based page coloring to work in this case, we need at least 32 colors. This means we need at least 5 bits of the cache index to intersect with the physical page number.\n\nCache line Tag    Cache Index   Bytes in Block\n\n                             5 bits\n\nPhysical Page Number            Page Offset\n\nSo, with associativity A, page size P, the minimum cache size is given by,\nC \u2265 A \u00d7 25 \u00d7 P = A \u00d7 32 \u00d7 P\n\nC \u2265 A \u00d7 32 \u00d7 8KB = A \u00d7 256KB\nMinimum cache size (associativity = 1) is 256KB",
        "solution_figures": [],
        "correctly_parsed": null
    },
    {
        "question_id": "final-fs2018-sol/Problem_4/b",
        "context": "Multicore Cache Partitioning [55 points]\n\nSuppose we have a system with 32 cores that share a physical second-level cache. Assume each core is running a single single-threaded application, and all 32 cores are concurrently running applications. Assume that the page size of the architecture is 8KB, the block size of the cache is 128 bytes, and the cache uses LRU replacement. We would like to ensure each application gets a dedicated space in this shared cache without any interference from other cores. We would like to enforce this using the OS-based page coloring mechanism to partition the cache, as we discussed in lecture. Recall that with page coloring, the operating system ensures, using virtual memory mechanisms, that the applications do not contend for the same space in the cache.\n",
        "context_figures": [],
        "question": "Assume the cache is 4MB, 32-way associative. Can the operating system ensure that the cache is partitioned such that no two applications interfere for cache space? Show your work.",
        "solution": "No.\n\nExplanation:\nFor a given associativity, minimum cache size = A \u00d7 256KB (from part a). Therefore, for a 32-way associative cache, minimum cache size required for the OS to ensure partitioning without interference is 32 \u00d7 256KB = 8MB. Since the cache size is only 4MB, the OS, in this case, cannot ensure partitioning without interference.",
        "solution_figures": [],
        "correctly_parsed": null
    },
    {
        "question_id": "final-fs2018-sol/Problem_4/c",
        "context": "Multicore Cache Partitioning [55 points]\n\nSuppose we have a system with 32 cores that share a physical second-level cache. Assume each core is running a single single-threaded application, and all 32 cores are concurrently running applications. Assume that the page size of the architecture is 8KB, the block size of the cache is 128 bytes, and the cache uses LRU replacement. We would like to ensure each application gets a dedicated space in this shared cache without any interference from other cores. We would like to enforce this using the OS-based page coloring mechanism to partition the cache, as we discussed in lecture. Recall that with page coloring, the operating system ensures, using virtual memory mechanisms, that the applications do not contend for the same space in the cache.\nAssume you would like to design a 32MB shared cache such that the operating system has the ability to ensure that the cache is partitioned such that no two applications interfere for cache space.",
        "context_figures": [],
        "question": "",
        "solution": "",
        "solution_figures": [],
        "correctly_parsed": null
    },
    {
        "question_id": "final-fs2018-sol/Problem_4/d",
        "context": "Multicore Cache Partitioning [55 points]\n\nSuppose we have a system with 32 cores that share a physical second-level cache. Assume each core is running a single single-threaded application, and all 32 cores are concurrently running applications. Assume that the page size of the architecture is 8KB, the block size of the cache is 128 bytes, and the cache uses LRU replacement. We would like to ensure each application gets a dedicated space in this shared cache without any interference from other cores. We would like to enforce this using the OS-based page coloring mechanism to partition the cache, as we discussed in lecture. Recall that with page coloring, the operating system ensures, using virtual memory mechanisms, that the applications do not contend for the same space in the cache.\n",
        "context_figures": [],
        "question": "Suppose we decide to change the cache design and use utility based cache partitioning (UCP) to partition the cache, instead of OS-based page coloring. Assume we would like to design a 4MB cache with a 128-byte block size. What is the minimum associativity of the cache such that each application is guaranteed a minimum amount of space without interference? Recall that UCP aims to minimize the cache miss rate by allocating more cache ways to applications that obtain the most benefit from more ways, as we discussed in lecture.",
        "solution": "Minimum associativity = 32.\n\nExplanation:\nUtility based cache partitioning needs to give at least one way for each application. Otherwise, the application will receive no cache space. Hence, the minimum associativity is 32.",
        "solution_figures": [],
        "correctly_parsed": null
    },
    {
        "question_id": "final-fs2018-sol/Problem_4/e",
        "context": "Multicore Cache Partitioning [55 points]\n\nSuppose we have a system with 32 cores that share a physical second-level cache. Assume each core is running a single single-threaded application, and all 32 cores are concurrently running applications. Assume that the page size of the architecture is 8KB, the block size of the cache is 128 bytes, and the cache uses LRU replacement. We would like to ensure each application gets a dedicated space in this shared cache without any interference from other cores. We would like to enforce this using the OS-based page coloring mechanism to partition the cache, as we discussed in lecture. Recall that with page coloring, the operating system ensures, using virtual memory mechanisms, that the applications do not contend for the same space in the cache.\n",
        "context_figures": [],
        "question": "Is it desirable to implement UCP on a cache with this minimum associativity? Why, why not? Explain.",
        "solution": "No, it is not desirable to implement UCP.\n\nExplanation:\nThere will be no benefit gained from UCP since UCP guarantees at least one way per application. This means all applications will be allocated exactly one way of the cache, i.e. the cache is equally and statically partitioned regardless of applications' utility for caching.",
        "solution_figures": [],
        "correctly_parsed": null
    },
    {
        "question_id": "final-fs2018-sol/Problem_4/f",
        "context": "Multicore Cache Partitioning [55 points]\n\nSuppose we have a system with 32 cores that share a physical second-level cache. Assume each core is running a single single-threaded application, and all 32 cores are concurrently running applications. Assume that the page size of the architecture is 8KB, the block size of the cache is 128 bytes, and the cache uses LRU replacement. We would like to ensure each application gets a dedicated space in this shared cache without any interference from other cores. We would like to enforce this using the OS-based page coloring mechanism to partition the cache, as we discussed in lecture. Recall that with page coloring, the operating system ensures, using virtual memory mechanisms, that the applications do not contend for the same space in the cache.\n",
        "context_figures": [],
        "question": "What is the maximum associativity of a 4MB cache that uses UCP such that each application is guaranteed a minimum amount of space without interference?",
        "solution": "32k ways.\n\nExplanation:\nThe maximum associativity corresponds to a fully associative design. For the given configuration, it is 4 MB / 128 bytes = 222 / 27 = 215 = 32k ways.",
        "solution_figures": [],
        "correctly_parsed": null
    },
    {
        "question_id": "final-fs2018-sol/Problem_4/g",
        "context": "Multicore Cache Partitioning [55 points]\n\nSuppose we have a system with 32 cores that share a physical second-level cache. Assume each core is running a single single-threaded application, and all 32 cores are concurrently running applications. Assume that the page size of the architecture is 8KB, the block size of the cache is 128 bytes, and the cache uses LRU replacement. We would like to ensure each application gets a dedicated space in this shared cache without any interference from other cores. We would like to enforce this using the OS-based page coloring mechanism to partition the cache, as we discussed in lecture. Recall that with page coloring, the operating system ensures, using virtual memory mechanisms, that the applications do not contend for the same space in the cache.\n",
        "context_figures": [],
        "question": "Is it desirable to implement UCP on a cache with this maximum associativity? Why, why not? Explain.",
        "solution": "No.\n\nExplanation:\nIt is not desirable to implement UCP with this maximum associativity because the overhead of UCP for 32 applications on this cache will likely outweigh its benefits. UCP will only work with LRU replacement policy. But implementing LRU on top of a 32k-way cache is impractical. Also the number of counters needed by UCP and the partitioning solution space for UCP are very large for such a cache.",
        "solution_figures": [],
        "correctly_parsed": null
    },
    {
        "question_id": "final-fs2018-sol/Problem_5/a",
        "context": "We have a system with 4 byte-addressable processors {P0, P1, P2, P3}. Each processor has a private 256-byte, direct-mapped, write-back L1 cache with a block size of 64 bytes. All caches are connected to and actively snoop a global bus, and cache coherence is maintained using the MESI protocol, as we discussed in class. Note that on a write to a cache block in the S state, the block will transition directly to the M state. Accessible memory addresses range from 0x00000 \u2212 0xfffff.\n\nEach processor executes the following instructions in a sequentially consistent manner:\n\nP0\n0 st r0, 0x1ff40\n-\n-\n\nP1\n1 st r0, 0x110c0\n2 st r1, 0x11080\n3 ld r2, 0x1ff00\n\nP2\n4 ld r0, 0x1ff40\n5 ld r1, 0x110f0\n-\n\nP3\n-\n-\n-\n\nAfter executing the above 6 memory instructions, the final tag store state of each cache is as follows:\n\nFinal Tag Store States\n\nCache for P0\nTag MESI state\nSet 0 0x1ff S\nSet 1 0x1ff S\nSet 2 0x110 I\nSet 3 0x110 I\n\nCache for P1\nTag MESI state\nSet 0 0x1ff S\nSet 1 0x1ff I\nSet 2 0x110 M\nSet 3 0x110 M\n\nCache for P2\nTag MESI state\nSet 0 0x10f I\nSet 1 0x1ff S\nSet 2 0x10f M\nSet 3 0x110 I\n\nCache for P3\nTag MESI state\nSet 0 0x133 E\nSet 1 0x000 I\nSet 2 0x000 I\nSet 3 0x10f I\n",
        "context_figures": [
            "images\\final-fs2018-sol\\chart_p14_0.png",
            "images\\final-fs2018-sol\\chart_p14_1.png",
            "images\\final-fs2018-sol\\chart_p14_2.png"
        ],
        "question": "Fill in the following tables with the initial tag store states (i.e., Tag and MESI state) before having executed the six memory instructions shown above. Answer X if a tag value is unknown, and for the MESI states, write in all possible values (i.e., M, E, S, and/or I).",
        "solution": "Initial Tag Store States\n\nCache for P0\nTag MESI state\nSet 0 0x1ff M, E, S\nSet 1 X M, E, S, I\nSet 2 0x110 M, E, S, I\nSet 3 0x110 M, E, S, I\n\nCache for P1\nTag MESI state\nSet 0 X M, E, S, I\nSet 1 0x1ff M, E, S, I\nSet 2 X M, E, S, I\nSet 3 X M, E, S, I\n\nCache for P2\nTag MESI state\nSet 0 0x10f I\nSet 1 X M, E, S, I\nSet 2 0x10f M\nSet 3 X M, E, S, I\n\nCache for P3\nTag MESI state\nSet 0 0x133 E\nSet 1 0x000 I\nSet 2 0x000 I\nSet 3 0x10f I",
        "solution_figures": [
            "images\\final-fs2018-sol\\chart_p14_3.png",
            "images\\final-fs2018-sol\\chart_p14_4.png"
        ],
        "correctly_parsed": null
    },
    {
        "question_id": "final-fs2018-sol/Problem_5/b",
        "context": "We have a system with 4 byte-addressable processors {P0, P1, P2, P3}. Each processor has a private 256-byte, direct-mapped, write-back L1 cache with a block size of 64 bytes. All caches are connected to and actively snoop a global bus, and cache coherence is maintained using the MESI protocol, as we discussed in class. Note that on a write to a cache block in the S state, the block will transition directly to the M state. Accessible memory addresses range from 0x00000 \u2212 0xfffff.\n\nEach processor executes the following instructions in a sequentially consistent manner:\n\nP0\n0 st r0, 0x1ff40\n-\n-\n\nP1\n1 st r0, 0x110c0\n2 st r1, 0x11080\n3 ld r2, 0x1ff00\n\nP2\n4 ld r0, 0x1ff40\n5 ld r1, 0x110f0\n-\n\nP3\n-\n-\n-\n\nAfter executing the above 6 memory instructions, the final tag store state of each cache is as follows:\n\nFinal Tag Store States\n\nCache for P0\nTag MESI state\nSet 0 0x1ff S\nSet 1 0x1ff S\nSet 2 0x110 I\nSet 3 0x110 I\n\nCache for P1\nTag MESI state\nSet 0 0x1ff S\nSet 1 0x1ff I\nSet 2 0x110 M\nSet 3 0x110 M\n\nCache for P2\nTag MESI state\nSet 0 0x10f I\nSet 1 0x1ff S\nSet 2 0x10f M\nSet 3 0x110 I\n\nCache for P3\nTag MESI state\nSet 0 0x133 E\nSet 1 0x000 I\nSet 2 0x000 I\nSet 3 0x10f I\n",
        "context_figures": [
            "images\\final-fs2018-sol\\chart_p14_0.png",
            "images\\final-fs2018-sol\\chart_p14_1.png",
            "images\\final-fs2018-sol\\chart_p14_2.png"
        ],
        "question": "In what order did the memory operations enter the coherence bus?",
        "solution": "time \u2192\n0 4 5 1 2 3",
        "solution_figures": [
            "images\\final-fs2018-sol\\chart_p14_5.png"
        ],
        "correctly_parsed": null
    },
    {
        "question_id": "final-fs2018-sol/Problem_6/a",
        "context": "A programmer writes the following two C code segments. She wants to run them concurrently on a multicore processor, called SC, using two different threads, each of which will run on a different core. The processor implements sequential consistency, as we discussed in the lecture.\n\nThread T0\nInstr. T0.0 X[0] = 1;\nInstr. T0.1 X[0] += 1;\nInstr. T0.2 while(flag[0] == 0);\nInstr. T0.3 a = X[0];\nInstr. T0.4 X[0] = a * 2;\n\nThread T1\nInstr. T1.0 X[0] = 0;\nInstr. T1.1 flag[0] = 1;\nInstr. T1.2 b = X[0];\n\nX and flag have been allocated in main memory, while a and b are contained in processor registers. A read or write to any of these variables generates a single memory request. The initial values of all memory locations and variables are 0. Assume each line of the C code segment of a thread is a single instruction.\n",
        "context_figures": [],
        "question": "What could be possible final values of a in the SC processor, after both threads finish execution? Explain your answer. Provide all possible values.",
        "solution": "0, 1, or 2.\n\nExplanation:\nThe sequential consistency model ensures that the operations of each individual thread are executed in the order specified by its program. Across threads, the ordering is enforced by the use of flag[0]. Thread 0 will remain in instruction T0.2 until flag is set by T1.1. There are at least three possible sequentially-consistent orderings that lead to at most three different values of a at the end:\nOrdering 1: T1.0 \u2192 T0.0 \u2192 T0.1 \u2192 T0.3 - Final value: a = 2.\nOrdering 2: T0.0 \u2192 T1.0 \u2192 T0.1 \u2192 T0.3 - Final value: a = 1.\nOrdering 3: T0.0 \u2192 T0.1 \u2192 T1.0 \u2192 T0.3 - Final value: a = 0.",
        "solution_figures": [],
        "correctly_parsed": null
    },
    {
        "question_id": "final-fs2018-sol/Problem_6/b",
        "context": "A programmer writes the following two C code segments. She wants to run them concurrently on a multicore processor, called SC, using two different threads, each of which will run on a different core. The processor implements sequential consistency, as we discussed in the lecture.\n\nThread T0\nInstr. T0.0 X[0] = 1;\nInstr. T0.1 X[0] += 1;\nInstr. T0.2 while(flag[0] == 0);\nInstr. T0.3 a = X[0];\nInstr. T0.4 X[0] = a * 2;\n\nThread T1\nInstr. T1.0 X[0] = 0;\nInstr. T1.1 flag[0] = 1;\nInstr. T1.2 b = X[0];\n\nX and flag have been allocated in main memory, while a and b are contained in processor registers. A read or write to any of these variables generates a single memory request. The initial values of all memory locations and variables are 0. Assume each line of the C code segment of a thread is a single instruction.\n",
        "context_figures": [],
        "question": "What could be possible final values of X[0] in the SC processor, after both threads finish execution? Explain your answer. Provide all possible values.",
        "solution": "0, 2, or 4.\n\nExplanation:\nThe value of X[0] is twice the value of a:\nOrdering 1: T1.0 \u2192 T0.0 \u2192 T0.1 \u2192 T0.3 \u2192 T0.4 - Final value: X[0] = 4.\nOrdering 2: T0.0 \u2192 T1.0 \u2192 T0.1 \u2192 T0.3 \u2192 T0.4 - Final value: X[0] = 2.\nOrdering 3: T0.0 \u2192 T0.1 \u2192 T1.0 \u2192 T0.3 \u2192 T0.4 - Final value: X[0] = 0.",
        "solution_figures": [],
        "correctly_parsed": null
    },
    {
        "question_id": "final-fs2018-sol/Problem_6/c",
        "context": "A programmer writes the following two C code segments. She wants to run them concurrently on a multicore processor, called SC, using two different threads, each of which will run on a different core. The processor implements sequential consistency, as we discussed in the lecture.\n\nThread T0\nInstr. T0.0 X[0] = 1;\nInstr. T0.1 X[0] += 1;\nInstr. T0.2 while(flag[0] == 0);\nInstr. T0.3 a = X[0];\nInstr. T0.4 X[0] = a * 2;\n\nThread T1\nInstr. T1.0 X[0] = 0;\nInstr. T1.1 flag[0] = 1;\nInstr. T1.2 b = X[0];\n\nX and flag have been allocated in main memory, while a and b are contained in processor registers. A read or write to any of these variables generates a single memory request. The initial values of all memory locations and variables are 0. Assume each line of the C code segment of a thread is a single instruction.\n",
        "context_figures": [],
        "question": "What could be possible final values of b in the SC processor, after both threads finish execution? Explain your answer. Provide all possible values.",
        "solution": "0, 1, 2, or 4.\n\nExplanation:\nBecause there are no specific instructions to enforce the execution ordering of T1.2, b can have any of the values that X[0] can have during the execution of the two threads.",
        "solution_figures": [],
        "correctly_parsed": null
    },
    {
        "question_id": "final-fs2018-sol/Problem_6/d",
        "context": "A programmer writes the following two C code segments. She wants to run them concurrently on a multicore processor, called SC, using two different threads, each of which will run on a different core. The processor implements sequential consistency, as we discussed in the lecture.\n\nThread T0\nInstr. T0.0 X[0] = 1;\nInstr. T0.1 X[0] += 1;\nInstr. T0.2 while(flag[0] == 0);\nInstr. T0.3 a = X[0];\nInstr. T0.4 X[0] = a * 2;\n\nThread T1\nInstr. T1.0 X[0] = 0;\nInstr. T1.1 flag[0] = 1;\nInstr. T1.2 b = X[0];\n\nX and flag have been allocated in main memory, while a and b are contained in processor registers. A read or write to any of these variables generates a single memory request. The initial values of all memory locations and variables are 0. Assume each line of the C code segment of a thread is a single instruction.\n",
        "context_figures": [],
        "question": "The programmer wants a and b to have the same value at the end of the execution of both threads. The final value of a and b should be the same value as in the original program (i.e., the possible final values of a that you found in part (a)). What minimal changes should the programmer make to the program?\n(Hint: You can use more flags if necessary.)",
        "solution": "She needs two more flags to enforce ordering.\n\nExplanation:\nSince the final value should be the same as in the original program, we have to maintain the flag[0] in T1.1 and T0.2. Then, b should not be updated until X[0] has the value that will be stored in a. Thus, either before or after T0.3, we need to set a new flag (flag[1]) that will be checked by Thread 1 before updating b. Finally, we cannot update X[0] until b has its final value. Thread 1 will set flag[2] only after b is updated. The modified code will be as follows:\n\nThread T0\nInstr. T0.0 X[0] = 1;\nInstr. T0.1 X[0] += 1;\nInstr. T0.2 while(flag[0] == 0);\nInstr. T0.3 a = X[0];\nInstr. T0.4 flag[1] = 1;\nInstr. T0.5 while(flag[2] == 0);\nInstr. T0.6 X[0] = a * 2;\n\nThread T1\nInstr. T1.0 X[0] = 0;\nInstr. T1.1 flag[0] = 1;\nInstr. T1.2 while(flag[1] == 0);\nInstr. T1.3 b = X[0];\nInstr. T1.4 flag[2] = 1;",
        "solution_figures": [],
        "correctly_parsed": null
    },
    {
        "question_id": "final-fs2018-sol/Problem_7/a",
        "context": "You have been hired to accelerate ETH's student database. After profiling the system for a while, you found out that one of the most executed queries is to \"select the hometown of the students that are from Switzerland and speak German\". The attributes hometown, country, and language are encoded using a four-byte binary representation. The database has 32768 (215) entries, and each attribute is stored contiguously in memory. The database management system executes the following query:\n\n1 bool position_hometown[entries];\n2 for(int i = 0; i < entries; i++){\n3 if(students.country[i] == \"Switzerland\" && students.language[i] == \"German\"){\n4 position_hometown[i] = true;\n5 }\n6 else{\n7 position_hometown[i] = false;\n8 }\n9 }\nYou are running the above code on a single-core processor. Assume that:\n\n\u2022 Your processor has an 8 MB direct-mapped cache, with a cache line of 64 bytes.\n\n\u2022 A hit in this cache takes one cycle and a miss takes 100 cycles for both load and store operations.\n\n\u2022 All load/store operations are serialized, i.e., the latency of multiple memory requests cannot be overlapped.\n\n\u2022 The starting addresses of students.country, students.language, and position_hometown are 0x05000000, 0x06000000, 0x07000000 respectively.\n\n\u2022 The execution time of a non-memory instruction is zero (i.e., we ignore its execution time).",
        "context_figures": [
            "images\\final-fs2018-sol\\chart_p17_0.png"
        ],
        "question": "How many cycles are required to run the query? Show your work.",
        "solution": "Cycles = cache_hits\u00d71 + cache_misses\u00d7100 = 0\u00d71 + (3\u00d732\u00d71024)\u00d7100\n\nExplanation:\nSince the cache size is 8 MB (223), direct-mapped, and the block size is 64 bytes (26), the\naddress is divided as:\n\n\u2022 block = address[5:0]\n\u2022 index = address[22:6]\n\u2022 tag = address[31:23]\n\nThe loop repeats for the total number of entries in the database (32\u00d71024 times). In\neach iteration, the code loads addresses 0x05000000 and 0x06000000. It also stores the\ncomputation at address 0x07000000 (three memory accesses in total per cycle). All three\naddresses have the same index bits, but different tags. The cache hit rate is 0% since every\nmemory access causes the eviction of the cache line that was just loaded into the cache.",
        "solution_figures": [],
        "correctly_parsed": null
    },
    {
        "question_id": "final-fs2018-sol/Problem_7/b",
        "context": "You have been hired to accelerate ETH's student database. After profiling the system for a while, you found out that one of the most executed queries is to \"select the hometown of the students that are from Switzerland and speak German\". The attributes hometown, country, and language are encoded using a four-byte binary representation. The database has 32768 (215) entries, and each attribute is stored contiguously in memory. The database management system executes the following query:\n\n1 bool position_hometown[entries];\n2 for(int i = 0; i < entries; i++){\n3 if(students.country[i] == \"Switzerland\" && students.language[i] == \"German\"){\n4 position_hometown[i] = true;\n5 }\n6 else{\n7 position_hometown[i] = false;\n8 }\n9 }\nRecall that in class we discussed AMBIT, which is a DRAM design that can greatly accelerate Bulk Bitwise Operations by providing the ability to perform bitwise AND/OR/XOR of two rows in a subarray. AMBIT works by issuing back-to-back ACTIVATE (A) and PRECHARGE (P) operations. For example, to compute AND, OR, and XOR operations, AMBIT issues the sequence of commands described in the table below (e.g., AAP (X,Y ) represents double row activation of rows X and Y followed by a precharge operation, AAAP (X,Y, Z) represents triple row activation of rows X, Y, and Z followed by a precharge operation).\n\nIn those instructions, AMBIT copies the source rows Di and Dj to auxiliary rows (Bi). Control rows Ci dictate which operation (AND/OR) AMBIT executes. The DRAM rows with dual-contact cells (i.e., rows DCCi) are used to perform the bitwise NOT operation on the data stored in the row. Basically, copying a source row to DCCi flips all bits in the source row and stores the result in both the source row and DCCi. Assume that:\n\n\u2022 The DRAM row size is 8 Kbytes.\n\n\u2022 An ACTIVATE command takes 50 cycles to execute.\n\n\u2022 A PRECHARGE command takes 20 cycles to execute.\n\n\u2022 DRAM has a single memory bank.\n\n\u2022 The syntax of an AMBIT operation is: bbop_[and/or/xor] destination, source_1, source_2.\n\n\u2022 Addresses 0x08000000 and 0x09000000 are used to store partial results.\n\n\u2022 The rows at addresses 0x0A000000 and 0x0B00000 store the codes for \"Switzerland\" and \"German\", respectively, in each four bytes throughout the entire row.",
        "context_figures": [
            "images\\final-fs2018-sol\\chart_p17_0.png",
            "images\\final-fs2018-sol\\chart_p18_0.png"
        ],
        "question": "i) The following code aims to execute the query \"select the hometown of the students that are from Switzerland and speak German\" in terms of Boolean operations to make use of AMBIT. Fill in the blank boxes such that the algorithm produces the correct result. Show your work.\n\n1 for(int i = 0; i < ; i++){\n2\n3 bbop_ 0x08000000, 0x05000000 + i*8192, 0x0A000000;\n4\n5 bbop_ 0x09000000, 0x06000000 + i*8192, 0x0B000000;\n6\n7 bbop_ 0x07000000, 0x08000000, 0x09000000;\n8 }\n\nii) How much speedup does AMBIT provide over the baseline processor when executing the same query? Show your work.",
        "solution": "i) 1st box = Number of iterations = database_size\nrow_buffer_size = 32\u22171024\u22174 bytes\n8\u22171024 bytes = 16\n2nd box = bbop_xor\n3rd box = bbop_xor\n4th box = bbop_or\n\nExplanation:\nAMBIT can execute the query as follows:\nT1 = country XOR \"Switzerland\"\nT2 = language XOR \"German\"\nhometown = T1 OR T2\n\nT1 and T2 are auxiliary rows used to store partial results.\n\nii) Speedup = 3\u00d7100\u00d732\u00d71024\n16\u00d72\u00d7(25\u00d750+11\u00d720)+16\u00d7(11\u00d750+5\u00d720)\n\nExplanation:\nTo compute an XOR operation, AMBIT emits 25 ACTIVATE and 11 PRECHARGE\ncommands. To compute an OR operation, it sends 11 ACTIVATE and 5\nPRECHARGE commands.",
        "solution_figures": [],
        "correctly_parsed": null
    },
    {
        "question_id": "final-fs2018-sol/Problem_8/a",
        "context": "An inexperienced CUDA programmer is trying to optimize her first GPU kernel for performance. After writing the first version of the kernel, she wants to find the best execution configuration (i.e., grid size and block size).\n\nAs she assigns one thread per input element, calculating the grid size (i.e., total number of blocks) is trivial. For N input elements, the grid size is d N/block_sizee, where block_size is the number of threads per block. So, the challenging part will be to figure out what is the block size that produces the best performance. She will try 5 different block sizes (64, 128, 256, 512, and 1024 threads).\n\nShe has learned that a general recommendation for kernel optimization is to maximize the occupancy of the GPU cores, i.e., Streaming Multiprocessors (SMs). Occupancy is defined as the ratio of active threads to the maximum possible number of active threads per SM.\n\nIn order to calculate the occupancy, it is necessary to take the available SM resources into account. She knows that in each SM of her GPU:\n\n\u2022 The total scratchpad memory or shared memory is 16 KB.\n\n\u2022 The total number of 4-byte registers is 16384.\n\nIn her first version of the kernel code, each thread needs 2 4-byte elements in shared memory for its private use. In addition, each block needs 10 4-byte elements in shared memory for communication across threads.\n\nShe has also learned that she can obtain the number of registers that each thread needs by using a special compiler flag. This way, she finds that each thread in the first version of the kernel uses 9 registers.\n",
        "context_figures": [],
        "question": "After reasoning some time about the amount of shared memory that her code needs, she decides to first test a block size of 128 threads. Why do you think she chose that number? Show your work.",
        "solution": "She calculated the maximum number of threads that an SM can hold according to the shared memory usage. 128 threads per block results in that maximum.\n\nExplanation:\nGiven the shared memory needs of her code, each block uses 2 \u00d7 block_size + 10 4-byte elements, that is, 4\u00d7 (2\u00d7 block_size+ 10) bytes.\nSince the shared memory available per SM is 16 KB, the number of blocks and the number of threads that each SM can allocate is as follows:\n\nblock_size Blocks/SM Threads/SM\n64 29 1856\n128 15 1920\n256 7 1792\n512 3 1536\n1024 1 1024\n\nShe decides to test a block size of 128 threads because this achieves the highest number of active threads per SM (i.e., the highest occupancy).",
        "solution_figures": [
            "images\\final-fs2018-sol\\chart_p20_0.png"
        ],
        "correctly_parsed": null
    },
    {
        "question_id": "final-fs2018-sol/Problem_8/b",
        "context": "An inexperienced CUDA programmer is trying to optimize her first GPU kernel for performance. After writing the first version of the kernel, she wants to find the best execution configuration (i.e., grid size and block size).\n\nAs she assigns one thread per input element, calculating the grid size (i.e., total number of blocks) is trivial. For N input elements, the grid size is d N/block_sizee, where block_size is the number of threads per block. So, the challenging part will be to figure out what is the block size that produces the best performance. She will try 5 different block sizes (64, 128, 256, 512, and 1024 threads).\n\nShe has learned that a general recommendation for kernel optimization is to maximize the occupancy of the GPU cores, i.e., Streaming Multiprocessors (SMs). Occupancy is defined as the ratio of active threads to the maximum possible number of active threads per SM.\n\nIn order to calculate the occupancy, it is necessary to take the available SM resources into account. She knows that in each SM of her GPU:\n\n\u2022 The total scratchpad memory or shared memory is 16 KB.\n\n\u2022 The total number of 4-byte registers is 16384.\n\nIn her first version of the kernel code, each thread needs 2 4-byte elements in shared memory for its private use. In addition, each block needs 10 4-byte elements in shared memory for communication across threads.\n\nShe has also learned that she can obtain the number of registers that each thread needs by using a special compiler flag. This way, she finds that each thread in the first version of the kernel uses 9 registers.\nHowever, after testing other block sizes, she finds out that using 256 threads per block provides higher performance than using 128. She does not understand why, so she looks for some information in the documentation of her GPU that can lead her to an explanation. There, she finds two more SM hardware constraints. In each SM:\n\n\u2022 The maximum number of blocks is 8.\n\n\u2022 The maximum number of threads is 2048.\n\nTake into account these new constraints when answering parts (b), (c), and (d).",
        "context_figures": [],
        "question": "Can you explain why using 256 threads per block perform better than 128? Show your work.",
        "solution": "The limitation in the maximum number of blocks per SM makes that the configuration with the highest occupancy is 256 threads per block.\n\nExplanation:\nThis is the corrected table after taking the limitation in the maximum number of blocks into account:\n\nblock_size Blocks/SM Threads/SM\n64 8 512\n128 8 1024\n256 7 1792\n512 3 1536\n1024 1 1024",
        "solution_figures": [
            "images\\final-fs2018-sol\\chart_p21_0.png"
        ],
        "correctly_parsed": null
    },
    {
        "question_id": "final-fs2018-sol/Problem_8/c",
        "context": "An inexperienced CUDA programmer is trying to optimize her first GPU kernel for performance. After writing the first version of the kernel, she wants to find the best execution configuration (i.e., grid size and block size).\n\nAs she assigns one thread per input element, calculating the grid size (i.e., total number of blocks) is trivial. For N input elements, the grid size is d N/block_sizee, where block_size is the number of threads per block. So, the challenging part will be to figure out what is the block size that produces the best performance. She will try 5 different block sizes (64, 128, 256, 512, and 1024 threads).\n\nShe has learned that a general recommendation for kernel optimization is to maximize the occupancy of the GPU cores, i.e., Streaming Multiprocessors (SMs). Occupancy is defined as the ratio of active threads to the maximum possible number of active threads per SM.\n\nIn order to calculate the occupancy, it is necessary to take the available SM resources into account. She knows that in each SM of her GPU:\n\n\u2022 The total scratchpad memory or shared memory is 16 KB.\n\n\u2022 The total number of 4-byte registers is 16384.\n\nIn her first version of the kernel code, each thread needs 2 4-byte elements in shared memory for its private use. In addition, each block needs 10 4-byte elements in shared memory for communication across threads.\n\nShe has also learned that she can obtain the number of registers that each thread needs by using a special compiler flag. This way, she finds that each thread in the first version of the kernel uses 9 registers.\n",
        "context_figures": [],
        "question": "What is the occupancy limitation due to register usage, if any? Explain and show your work.",
        "solution": "There is no occupancy limitation due to the register usage.\n\nExplanation:\nThe register usage depends on the block size. As she knows the number of registers per thread (9), she can calculate the total register needs:\n\nblock_size Blocks/SM Threads/SM Registers/block Registers/SM\n64 8 512 576 4608\n128 8 1024 1152 9216\n256 7 1792 2304 16128\n512 3 1536 4608 13824\n1024 1 1024 9216 9216\n\nIn all cases, the total register usage is lower than 16384. (NOTE: The solution is correct if only calculated for 256 threads.)",
        "solution_figures": [
            "images\\final-fs2018-sol\\chart_p21_1.png"
        ],
        "correctly_parsed": null
    },
    {
        "question_id": "final-fs2018-sol/Problem_8/d",
        "context": "An inexperienced CUDA programmer is trying to optimize her first GPU kernel for performance. After writing the first version of the kernel, she wants to find the best execution configuration (i.e., grid size and block size).\n\nAs she assigns one thread per input element, calculating the grid size (i.e., total number of blocks) is trivial. For N input elements, the grid size is d N/block_sizee, where block_size is the number of threads per block. So, the challenging part will be to figure out what is the block size that produces the best performance. She will try 5 different block sizes (64, 128, 256, 512, and 1024 threads).\n\nShe has learned that a general recommendation for kernel optimization is to maximize the occupancy of the GPU cores, i.e., Streaming Multiprocessors (SMs). Occupancy is defined as the ratio of active threads to the maximum possible number of active threads per SM.\n\nIn order to calculate the occupancy, it is necessary to take the available SM resources into account. She knows that in each SM of her GPU:\n\n\u2022 The total scratchpad memory or shared memory is 16 KB.\n\n\u2022 The total number of 4-byte registers is 16384.\n\nIn her first version of the kernel code, each thread needs 2 4-byte elements in shared memory for its private use. In addition, each block needs 10 4-byte elements in shared memory for communication across threads.\n\nShe has also learned that she can obtain the number of registers that each thread needs by using a special compiler flag. This way, she finds that each thread in the first version of the kernel uses 9 registers.\n",
        "context_figures": [],
        "question": "The performance obtained by the first kernel version does not fulfill the acceleration needs. Thus, the programmer writes a second kernel version that reduces the number of instructions at the expense of using one more register per thread. What would be the highest occupancy for the second kernel? For what block size(s)?",
        "solution": "The highest occupancy will be 1536/2048 = 0.75. It can be obtained with blocks of 256 or 512 threads.\n\nExplanation:\nThe number of registers per thread is 10 in the second kernel. The configuration with 256 threads per block for the second kernel version will be able to allocate one less block per SM (6) than for the first kernel version (7):\n\nblock_size Blocks/SM Threads/SM Registers/block Registers/SM\n64 8 512 640 5120\n128 8 1024 1280 10240\n256 6 1536 2560 15360\n512 3 1536 5120 15360\n1024 1 1024 10240 10240",
        "solution_figures": [
            "images\\final-fs2018-sol\\chart_p22_0.png"
        ],
        "correctly_parsed": null
    }
]